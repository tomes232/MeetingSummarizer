Yeah, still doesn't feel natural. I I wish, you know, the the the the room would just detect when an when a meeting's started it starting. Voila cool. So you better start. It was your idea. So. Well maybe uh is is the white-board working? Or or not. Um y maybe show us the general sort of outline. You keep w okay, you can just write it on the paper. Yeah. So is this different from the original P_L_P_ two stuff then, is that? Uh 'cause that's what you started with, wasn't it? This this stuff that Marius Okay. Okay. Yeah, yeah. H he's parameterizing it both temporally and across the spectrum, yep. Yeah. Yeah. Yep, okay. Yep. Yeah, to that. Yep, yep, yep. Mm yeah, but it's a it's a it's a different linear prediction model, isn't it. It's a complex linear prediction or whatever. Yeah. Yeah. Yeah. Yeah. Yep. Okay. Okay. You need the phase. Ooh uh Well Well it depends on how this i I mean you're talking b yeah, just the carrier. Okay, so it's not necessarily phase, but if you're using some sort of sinusoidal coding, it's phase, yeah. Okay. Okay. W What so what sort of bit rate do you get for the um encoding of the this the Hilbert transform? The s the spectrogram. W what sort of coding do you get so far for the no no no no, f f yeah. Yeah. Yeah, yep. Uh wo well, it's no no no, but what sort of bit rate do you get? Is it is it uh id do you have any sort of m idea of what sort of band-widths? Yeah. 'Cause Okay, and that's in that's with the fully optimised sort of compression scheme or that's just the raw data or? Okay, yeah. Yeah. Sure, sure. You just cut out m bands and just use interpolation or whatever. Yeah. T so that's one point okay, that's with the full spectrum basically being used then. Yeah. Yep, yep. Yep. Yep. Yeah, yep. Okay, that's quite good. So and Hynek was showing me how scalable it is in terms of um you can take half the bands away and still get decent performance. You take you take three quarters of them away, it starts to degrade, but it's still intelligible, that sort of thing. Yep. Okay. And that's without doing any sort of um prediction of what the missing bands might be, right? You just leave them out. I mean you you shou you could be able to do some sort of code missing data sort of Bayes stuff, yeah. Okay. Okay. Yeah. It it would be it would be nice to have a scalable scheme though, wouldn't it? Like because you've got a nice scalable scheme for the for for the the coding of the the smooth spectrum. It'd be nice to have some sort of scalable coding scheme for the for the voice signal as well, right? Well, I don't know, I'm just Well, at different bit rates. So the actual uh excitation as well. Because I mean it'd be nice uh l let's say if you had twelve hundred bits for the for the sp smooth spectrum and twelve hundred bits for the voice, and then you could scale them both. Yeah. Sure, sure, sure. Yeah. Uh No no. Just long as it's some sort of non-linear. Yep. M The place more emphasis on the Yeah, yeah, yeah. Okay. Yeah. Okay. Hmm. And then vector quantize or whatever. Yeah. Hmm. Hmm. Mm-hmm. So times fifteen. Twen that's t it's twenty times fifteen, it's not Okay. So it's it's yeah, it's three hundred. It I mean it's it's total of three hundred sort of parameters per second. Um Well, y you're not you're not carrying transmitting that at all really at this point in time. I uh And and you get whispering type synthesis, yeah. Yep. Or one of these glottal pulse things. F and just use basic L_P_C_ ten style. Yeah, which is which is probably, I mean, if you're going to aim for like a very low rate code to below five hundred bits per second, I guess. That's that's probably fine. But Uh the lowest standard is MELP I think, which is twenty four hundred, the standard. It's been implemented at twelve hundred f bits per second using more more complicated quantization schemes and stuff like that, I think. I think so, yeah. I s I saw that they used matrix quantisation on the L_P_ uh on the L_S_F_ parameters or whatever. But I mean that's quite it becomes quite computationally intensive. But I mean I imagine the same sort of approach could be taken with yours as well. No. Yeah, that's the same. Twenty MELP*'s a lot better quality though. Yeah. ..No no. I think I think I think I think CELP or CELP or however you pronounce that. It's about forty eight hundred I think. Yeah. Well the standard's forty eight hundred or something, yeah. Yeah. Or based around that. Yeah. Okay. It's not terribly interesting though, really. Yeah. So I'm Well you you've well, I mean you've got a bunch of options, haven't you. You can use like uh a codec sided or you could use a Sinusoidal based. No. Yeah, yeah. Sure, sure, sure. Yeah, but what's more interesting? I I mean 'cause c I I I suspect there somethings like um CELP. You're gonna get h lot higher band-width, aren't you. I mean it's a high band-width standard. So yeah. Okay. Yeah, yeah, yeah. Okay. Hmm. Okay. So I mean How Yep. Yeah. 'Cause it's a bit tricky, isn't it. That's it's not like a standard I'm just thinking, I mean what sort of opt it's not like you can't just do standard um, you know, inverse filtering for um signal reconstruction or anything like that, can you? Uh oh okay and then you just then you just do the inverse F_F_T_ or whatever. Okay, the oth I mean the other o Mm yeah. But that's gonna be susceptible to having phase problems, is that, when you Yeah, yeah. Yeah. Okay. Yeah. Hmm. And now I mean there is there are um mm there is research uh um pu that's been published showing, you know, speech reconstruction from the essentially the spectrogram without phase information. Yeah. So Yes. Yes, but I mean you can reconstruct the the f the th the entire Spectrogram from that, can't you. So you could do it that way. Cause I'm just saying there's a heaps of there heaps of options. Like if y you be exactly the same as sinusoidal coding or something like that. All you need is the reconstruction of the spectrogram in order to estimate the harmonic amplitudes and Yeah. Yeah. Ye Just by refitting. Yeah. Yeah, yeah. Okay. Yeah, okay. D it's th Yeah. But that's that's the big thing at the moment, isn't it. If if you're mm if i if the if the exci if the if the encoding rate's two kilo-bits per second for the the excitation component of the signal. It doesn't matter how efficient your um encoding of the spectrogram is, you're always gonna have a high relatively high bit rate.. Yeah. Nah. Yeah, yeah. Yeah, sure. Well, one could argue it's it's a field that's quite a sort of a niche these days, isn't it. Um In standards and yeah. Mm yeah. Well did you say uh who said Yeah. Is it Nokia's being sued in the U_S_ for their tri-band usage 'cause, you know, intellectual property. Uh no no, g G_S_M_ because they don't have G_S_M_ in the U_S_. Or I didn't have G_S_M_ in the U_S_. UH it's f uh okay. No no n Uh well that's tri-band, s yeah. Yeah, yeah. Yeah. Yeah. Yeah yep. And apparently Nokia's in a bit of trouble over that one, but yeah. Well it's interesting. Yeah. Should've been a lawyer. Well that's But I mean coding is a much more sort of it's it's it well I mean it's it's uh it when it boils down to just mathematics to a large extent, doesn't it. It's it's just uh sort of it's, you know, communications s sort of No. Yeah. Sure, sure. Yeah. Sure, sure. Push to talk and stuff like that. Yep. And I mean, as you say, I mean that delay is, I mean you can squash that delay to some extent, right. I n I think as far as the co you know, c c Delilah's coding goes, I don't think you can probably do much more than what's already been achieved. Because to get higher compression you have to, you know, take advantage of redundancy over longer time spans. So Yeah, sure, but I mean that's transmission delay, not algorithmic delay. If you add those together you end up with a the nightmare scenario, I guess. We might have someone else joining our uh meeting soon. Oh yeah. I don't know, I mean uh you'd be you tried sinusoidal coding based approaches, didn't you? Sinusoidal coding. Yeah, but sure. Yeah, m yeah, tr yeah it's a yeah, but I mean the th the the thing is you'd need ti it part of the parameter set for that is uh the um uh the f uh the harmonic amplitudes. And then one way of compressing the harmonic amplitudes is just storing the smooth spectrum. And then you can uh and then you can interpolate yeah, you can just pick the the um the harmonic amplitudes on that spectrum when you reconstruct on that smooth version of the spectrum when you reconstruct. Y yeah. Well, I was just saying Yeah. Yep. Yeah, well I I th it's high. It's quite a high bit rate. But I mean once we it maybe it's something that's scalable. Well it I mean you can use as many harmonics as you want. I m h uh he was just using the fundamental frequency. And if you smooth it doing a smooth spectrum representation of the harmonic amplitudes, then you only need to store the the the fundamental frequency and and the and the maximum voice frequency. And everything else can be yeah, s yeah, sure. So, and then but then the tricky the only tricky thing that we hadn't figured out was the phase information. Um but y but uh you can still get a like you get th um basically if you do phase unwrapping and all of these other tricks, you end up with some sort of phase representation, which is some sort of monotonically decreasing function. Yeah, unwrapped, yeah. So I mean it's something that you could conceivably represent in some sort of low sort of r you know, low bit rate representation, I guess. And then maybe you'd need some sort of correction to um to correct minor phase discontinuities at some point in time. 'Cau Yeah, yeah, sure. Yeah, okay. Okay. Yeah. Yeah. Okay. Yeah. Sure, sure. Yeah. Yep. Yeah, yeah. Hmm. No, and you uh you wanna avoid hard choices on voicing decision, yeah. Well that's one of the big problems with L_P_C_ ten for instance. I it stinks because this hard yeah. Yeah, sure. Yeah. Yeah, yep. But but that that two hundred this is what they use for like um st they they do use this for um for um pitch marking in speech. So this is what Alan Black's code uses. But it's very speaker dependent wh on what sort of band width you'd look at this Hilbert envelope. Depending on the pitch, depending on the person, y you'll get a different amount of smoothing and a certain sort of so I mean, you know, if it's a female speaker for instance you yeah. So it's sorta tricky. Yeah. Uh n no. Y yeah, but from memory that that basic script that they had wasn't wasn't very um effective. I mean maybe if you did some sort of initial first p guess and then and then chose your smoothing factor based on that. 'Cause it it you know, selecting at any particular point of what the what the pitch is at the may be inaccurate. But if you estimated it over a second, you should be able to get a decent estimate, I'd I'd think. Yeah. Well it'd be nice to it would be nice to really see what this what you're a actually trying to reconstruct here. This carrier signal, what what it really looks l yeah. And you did say it looks like a frequency modulated cosine. Hmm. Yeah. It would have to be 'cause pitch shifts yeah. Yeah. Yeah. Hmm. Hmm. Mm. Yeah. Mm-hmm. Hmm. Yeah, it's like a yeah. And it it does start to get a little bit n non-linear towards the end. But normally that's that's when the components are not actually probably gonna be audible anyway in voiced speech, right. So yeah. Yeah. Yeah. Yeah, sure. C C 'cause any sort of error in that in that line is is transmitted, is realised in a net as a phase error, right. Yeah. Mm. Okay. Mm yeah. Tricky, yeah. Yeah, yeah, yeah. Y y y you have s five slices like n across that way, I guess. Yeah. Hmm. Yeah, that makes sense 'cause it's a time domain sort of approach. Um and I mean uh and and in in fact I mean it's what you're doing is so different to most approaches to speech coding that it doesn't matter that we don't know much about or haven't had much i it's I mean it's quite different to a lot of the traditional sort uh so uh voi um you know, source filter type approaches or or whatever, yeah. So It's just a matter of minimum distortion time. Yeah. Sure. Unvoiced unvoiced speech is very difficult to b sort of judge on quality compared to voiced, I think. Plosives. Yeah, plosives are very important though, I guess. So maybe it does a better job of plosives. Okay. It'd be nice if we had some demos here. Well, I suppose have to wait to your I guess. Oh it's a little wa oh bugger. I won't be, yeah. No. You'll practice it before then, won't you? Yeah, he's going to Oz. Yep. I did, yeah. But we're d we're doing a more traditional s s frequency slices rather than temporal based stuff. Yeah. Well that's w that was one of the approaches, sure. Yep. And the other approach was was this H_M_M_ based stuff, which is which is basically just a parameterization of the, you know, the the the the um the source filter type parameters, and then and then learning the, you know, training like a h speech recognition system. But that's not terribly that's got no relationship to this. I it's yeah. Yeah, sure. Speech coding. I I read some of the other techniques. Um little standards. But um I mean I think I think you're on the right track with the analysis by synthesis but that doesn't tell you how what sort of bit rates you're going to achieve or um and because you have to do it separately for each um band, it would be the I imagine that's the tricky part. Um because it would be varying quite rapidly, wouldn't it, your um your uh code book. Like w within each um when you go along the the the b the one second sort of segment of the band, you Yeah, yeah. Yeah. Well, I mean yeah. First you've gotta choose your approach and not worry about the band-width, and then start thinking about how to given 'cause I mean I guess especially 'cause this is a this is a more of a commercial based project, you've g c you've you've gotta i match the quality before you start worrying about other things, right. I mean d d do they I mean what what what sort of uh Uh-huh, okay. Well that makes things uh With They're happy. Okay. Cool. Yeah. Sure, sure. And at the moment you're at sort of a point where maybe you have to make that choice a little bit. Yeah. Mm-hmm. Yeah. Uh which is not Yeah. Yeah. Al although the standards, depending on what standard you're dealing with, sometimes it can be quite ambiguous, right. I mean all the M_ PEG standards are basically this is the container, we don't care how you do it, right, but it's gotta fit inside this this sort of this sort of format. Okay. And then you have the v and then you have to do M_O_S_ um testing and Okay. Well, ho it Okay. Mm. Yeah, yeah. Well, I th No, no, no. Uh no, no, no. Yep. Well the the what's what's new is in the transmission medium. Like you know, new like higher band width transmission or uh or like packet packetized based transmission. And then all th that sort of thing. The where th where they're they're not so much looking at the algorithm, but so much as how to incorporate the algorithm within a new transi mission sort of s medium. Basically s like I mean if you d if you see a lot of papers on like um on uh voice over I_P_ or whatever, the the publications are sort of like um how do we deal with packet loss, or or how was it affected by packet loss and and and things like that. Yeah, sure. Yeah. And optimization of the code and all that sorta stuff. Yeah. But even that's less of a big deal now, right, because all the little um I_ s yeah. Not really. You could trow Matlab on a mobile phone these days. Well, maybe not, but Yeah. Y yeah. Yeah, which David did, yeah. It's good, yeah. Yeah, we've got all his code implemented. So uh It wasn't perfect. There was little bit of bugs with some of the matching. But it was pretty hard to tell that it was m not not perfect unless you heard the samples sort of next to one another. So that's not too bad. Oh, sure.. And I mean it Well th The only trick is the phase. And we've got a student starting in January who's gonna start working on on how to actually efficiently parameterize the um the H_M_M_ yeah, yeah. And I think I think we've got a number of ideas for the the spectrum. That's easily enough handled just using standard techniques. But the Without phase? I've got no idea. Uh you reviewed a paper on a Uh Steven Cox, isn't it? Yeah. University of East Anglia. It's near c sort of it's it's near the sorta Cambridge side of England, I guess. No the much more mathematical, I think. Yeah, just basically doing a search through or or stuff stuff like that, don't they? Or Yeah. Yeah. No. Yeah, yeah. Sure. Where you've only got the M_F_C_C_s. But th I think I think the I think it was to was it to do with Aurora or something? The Okay. I was Okay. Yeah, but it w it was for say um say you've got someone who has you know, they wanna dispute that this is what they actually said by the recognition. And the recognition network was wrong and you owe me money or whatever. Um and then they can say well we've we've reconstructed your speech from the speech recogni input to the speech recognizer. Hey voila it sounds like you uh Uh I mean obviously there's a lot more to it than that. There's a lot of sort of, well, does the quality stand up to legal sort of requirements or whatever. But um Oh no, that there's a separate set of standards I think for speech recognition. Y yeah, yeah, yeah. Yep. Which uh I think requires, you know, that states the un the information they can send is this. So I don't know whether you tack on any extra phase information or whatever. You have to deal with what you're given. So Yeah. I don't know. I I it would be interesting. You I mean maybe that's the best approach to look at to begin with is just pretend all you have got is Yeah. Um because you can also interpolate the pitch information from the the spectrum as well. They do that. Yeah. So I mean maybe try to see how good you can go without transmitting any of that information. The pitch was really good, I thought. I th didn't think you could really tell. Yeah. Yeah. Yeah, but but the thing is if it's Mel scaled, if you have a Mel scale, you get quite fine reconstruction of the spectrum at the lower frequencies it which is what you need for reconstruction of the pitch anyway. So I mean if you wanted to transmit with fewer bands, you'd maybe still keep the lower bands or or maybe you could still transmit the pitch. The pitch isn't that big a deal. But it'd be nice if you didn't because then I mean that's well, I mean it's worth giving it a go and then men maybe seeing wh what information you can add to the data stream to improve performance. Um I don't know. That that'd be cool. So get coding. Mm I think w Ah, well I mean Huffman's easy enough. But but yeah, sure. Uh that that's where I stop. Run length encoding, I'm out of here. Okay. I mean I I th I think they do try to I mean obviously the the the techniques they they choose are supposed to be less, you know, susceptible to coding errors, let's say. You know, i but I mean that's maybe the the limit to that. Yeah. Okay. Okay.